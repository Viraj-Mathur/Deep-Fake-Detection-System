# -*- coding: utf-8 -*-
"""Deep Fake Detection System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/153WWAnalFoE6kUYGb4UYtW6YZwBKmyws
"""

import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.models import load_model

"""## Loading Dataset

google collab Link : https://colab.research.google.com/drive/153WWAnalFoE6kUYGb4UYtW6YZwBKmyws
"""

# Define dataset paths (Local Paths)
train_dir = './Dataset/Train'
validation_dir = './Dataset/Validation'
test_dir = './Dataset/Test'

# Load and preprocess the data
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the ImageDataGenerators for each dataset
train_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Create the data flows for training, validation, and test sets
train_flow = train_datagen.flow_from_directory(
    train_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical'
)

validation_flow = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical'
)

test_flow = test_datagen.flow_from_directory(
    test_dir,
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical'
)

# Import necessary libraries
import os
import matplotlib.pyplot as plt
from matplotlib.image import imread
import seaborn as sns

# Set the style for seaborn
sns.set(style='whitegrid')

# Define the paths for Train and Test folders
train_path = './Dataset/Train'
test_path = './Dataset/Test'

# Define categories
categories = ['Fake', 'Real']

# Function to count the number of images in each category
def count_images(path, categories):
    data_count = {}
    for category in categories:
        folder = os.path.join(path, category)
        count = len(os.listdir(folder))
        data_count[category] = count
    return data_count

# Count images in Train and Test folders
train_counts = count_images(train_path, categories)
test_counts = count_images(test_path, categories)

# Display counts
print("Train Image Counts:", train_counts)
print("Test Image Counts:", test_counts)

# Function to display sample images from each category
def display_samples(path, categories, num_samples=5):
    for category in categories:
        folder = os.path.join(path, category)
        images = os.listdir(folder)

        plt.figure(figsize=(15, 5))
        plt.suptitle(f'Sample Images - {category}', fontsize=16)

        for i in range(num_samples):
            image_path = os.path.join(folder, images[i])
            plt.subplot(1, num_samples, i + 1)
            img = imread(image_path)
            plt.imshow(img)
            plt.axis('off')

        plt.show()

# Display samples from Train and Test sets
display_samples(train_path, categories)
display_samples(test_path, categories)

"""## Building a basic CNN Model"""

# Build the CNN model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_flow, epochs=10, validation_data=validation_flow)

# Plot training & validation loss values
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

# Plot training & validation accuracy values
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')

plt.show()

# # Evaluate the model
# loss, accuracy = model.evaluate(test_flow)
# print(f'Test Accuracy: {accuracy * 100:.2f}%')

model.save('./model/deepfake_new.h5')

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# Evaluate the model on the test data and print the confusion matrix
predictions = model.predict(test_flow)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_flow.classes
class_labels = list(test_flow.class_indices.keys())

# Confusion Matrix
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print("Confusion Matrix:\n", conf_matrix)

# Classification Report
report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print("Classification Report:\n", report)

# Calculate accuracy
accuracy = accuracy_score(true_classes, predicted_classes)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Optional: Compare with an existing model (if you have another trained model)
# Uncomment the following code if you want to compare with an existing model
# existing_model = load_model('/path/to/existing/model.h5')
# existing_model_predictions = existing_model.predict(test_flow)
# existing_predicted_classes = np.argmax(existing_model_predictions, axis=1)
# existing_accuracy = accuracy_score(true_classes, existing_predicted_classes)
# print(f"Existing Model Accuracy: {existing_accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# Predictions and metrics calculation
predictions = model.predict(test_flow)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = test_flow.classes
class_labels = list(test_flow.class_indices.keys())

# Confusion Matrix
conf_matrix = confusion_matrix(true_classes, predicted_classes)

# Plotting the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Classification Report
report = classification_report(true_classes, predicted_classes, target_names=class_labels, output_dict=True)

# Extracting metrics for each class
metrics_df = pd.DataFrame(report).transpose()
metrics_df = metrics_df.loc[class_labels, ['precision', 'recall', 'f1-score']]

# Plotting Precision, Recall, and F1-Score
metrics_df.plot(kind='bar', figsize=(10, 6))
plt.title("Precision, Recall, and F1-Score for Each Class")
plt.xlabel("Class")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.legend(loc="lower right")
plt.show()

# Accuracy over epochs (optional, only if you tracked it)
plt.figure(figsize=(10, 6))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()

# Loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.show()

# Get an image and its label from the test flow
img, label = next(test_flow)
label_ = label.argmax(axis=1)

label_

# Make predictions using the trained CNN model
res = model.predict(img)

res

# Determine the predicted class
class_ = res.argmax(axis=1)
class_

# Print the results based on the predicted and actual class
if class_[0] == 0:
    if label_[0] == 0:
        print("Actual class is fake, predicted class is fake")
    else:
        print("Actual class is real, predicted class is fake")
else:
    if label_[0] == 0:
        print("Actual class is fake, predicted class is real")
    else:
        print("Actual class is real, predicted class is real")

# Print the results based on the predicted and actual class
if class_[0] == 0:
    print("Predicted class is Fake")
else:
    print("Predicted class is Real")

# Display the image
plt.imshow(img[0])
plt.show()

model.save('./model/trained_model.h5')

len(class_)

"""## Checking CNN Model Prediction with Foreign Data"""

from PIL import Image
import numpy as np

from PIL import Image
import numpy as np

# Load the CNN model
cnn_model = tf.keras.models.load_model('./model/trained_model.h5')

# Define image paths
TRAIN_PATH_1 = './vyratkoli2/viratkohli.jpeg'
TEST_IMAGE_PATH = './vyratkoli2/tom-holland-back-to-the-future-deepfake.jpg'

# Function to preprocess the image and make a prediction
def predict_image(model, image_path):
    img = Image.open(image_path)
    img = img.resize((64, 64), Image.Resampling.NEAREST)
    img_array = np.array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Make prediction using the model
    res = model.predict(img_array)
    class_label = "Real" if np.argmax(res) == 0 else "Fake"
    print(f"Prediction for {image_path} is: {class_label} ({res[0][0]:.2f} confidence for Real)")

# Predict on the given images using the CNN model
predict_image(cnn_model, TRAIN_PATH_1)
predict_image(cnn_model, TEST_IMAGE_PATH)

"""## Preparing Resnet Model"""

# preparing resnet model

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# Load ResNet50 model pre-trained on ImageNet, excluding the top fully-connected layers
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))

# Freeze the base layers of ResNet to use it as a feature extractor
for layer in resnet_base.layers:
    layer.trainable = False

# Add custom layers on top of ResNet
resnet_model = Sequential([
    resnet_base,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax')
])

# Compile the ResNet model
resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train ResNet model
resnet_history = resnet_model.fit(train_flow, epochs=10, validation_data=validation_flow)

# Plot ResNet training & validation loss values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(resnet_history.history['loss'])
plt.plot(resnet_history.history['val_loss'])
plt.title('ResNet Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

# Plot ResNet training & validation accuracy values
plt.subplot(1, 2, 2)
plt.plot(resnet_history.history['accuracy'])
plt.plot(resnet_history.history['val_accuracy'])
plt.title('ResNet Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='lower right')

plt.show()

# Evaluate ResNet model
resnet_loss, resnet_accuracy = resnet_model.evaluate(test_flow)
print(f'ResNet Test Accuracy: {resnet_accuracy * 100:.2f}%')

"""## Checking the Prediction of ResNet Model on Foreign Data"""

from PIL import Image
import numpy as np

# Load the ResNet model
resnet_model = tf.keras.models.load_model('./model/deepfake_resnet.h5')

# Define image paths
TRAIN_PATH_1 = './vyratkoli2/viratkohli.jpeg'
TEST_IMAGE_PATH = './vyratkoli2/tom-holland-back-to-the-future-deepfake.jpg'

# Function to preprocess the image and make a prediction
def predict_image(model, image_path):
    img = Image.open(image_path)
    img = img.resize((64, 64), Image.Resampling.NEAREST)
    img_array = np.array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

    # Make prediction using the ResNet model
    res = model.predict(img_array)
    class_label = "Real" if np.argmax(res) == 0 else "Fake"
    print(f"Prediction for {image_path} is: {class_label} ({res[0][0]:.2f} confidence for Real)")

# Predict on the given images
predict_image(resnet_model, TRAIN_PATH_1)
predict_image(resnet_model, TEST_IMAGE_PATH)

# Save ResNet model
resnet_model.save('./model/deepfake_resnet.h5')

# Comparison of both models

# Predict using CNN model
cnn_predictions = model.predict(test_flow)
cnn_pred_classes = np.argmax(cnn_predictions, axis=1)

# Predict using ResNet model
resnet_predictions = resnet_model.predict(test_flow)
resnet_pred_classes = np.argmax(resnet_predictions, axis=1)

# Model comparison metrics
from sklearn.metrics import classification_report, confusion_matrix

# Classification Report and Confusion Matrix for CNN model
print("CNN Model Classification Report:\n", classification_report(true_classes, cnn_pred_classes, target_names=class_labels))
print("CNN Model Confusion Matrix:\n", confusion_matrix(true_classes, cnn_pred_classes))

# Classification Report and Confusion Matrix for ResNet model
print("ResNet Model Classification Report:\n", classification_report(true_classes, resnet_pred_classes, target_names=class_labels))
print("ResNet Model Confusion Matrix:\n", confusion_matrix(true_classes, resnet_pred_classes))

# Model performance comparison
print(f"CNN Model Test Accuracy: {accuracy * 100:.2f}%")
print(f"ResNet Model Test Accuracy: {resnet_accuracy * 100:.2f}%")

# Plot accuracy and loss comparison
plt.figure(figsize=(14, 6))

# Accuracy comparison
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='CNN Train')
plt.plot(history.history['val_accuracy'], label='CNN Validation')
plt.plot(resnet_history.history['accuracy'], label='ResNet Train')
plt.plot(resnet_history.history['val_accuracy'], label='ResNet Validation')
plt.title('Model Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss comparison
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='CNN Train')
plt.plot(history.history['val_loss'], label='CNN Validation')
plt.plot(resnet_history.history['loss'], label='ResNet Train')
plt.plot(resnet_history.history['val_loss'], label='ResNet Validation')
plt.title('Model Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""## Comparison of Both Models"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# Simulate test accuracy values
cnn_accuracy = 0.5022  # Fixed value as requested
resnet_accuracy = 0.6468  # Fixed value as requested

# Assuming true_classes is the true labels from test data
# Get predictions for both models
cnn_predictions = cnn_model.predict(test_flow)
cnn_pred_classes = np.argmax(cnn_predictions, axis=1)

resnet_predictions = resnet_model.predict(test_flow)
resnet_pred_classes = np.argmax(resnet_predictions, axis=1)

# Classification reports for both models
cnn_report = classification_report(true_classes, cnn_pred_classes, target_names=class_labels, output_dict=True)
resnet_report = classification_report(true_classes, resnet_pred_classes, target_names=class_labels, output_dict=True)

# Convert classification reports to DataFrames for visualization
cnn_df = pd.DataFrame(cnn_report).transpose()
resnet_df = pd.DataFrame(resnet_report).transpose()

# Combine detailed metrics for plotting
metrics = ['precision', 'recall', 'f1-score']

# Data preparation for side-by-side comparison
cnn_df['Model'] = 'CNN'
resnet_df['Model'] = 'ResNet'
cnn_df.reset_index(inplace=True)
resnet_df.reset_index(inplace=True)
combined_df = pd.concat([cnn_df[cnn_df['index'].isin(class_labels)],
                        resnet_df[resnet_df['index'].isin(class_labels)]])
combined_df.rename(columns={'index': 'Class'}, inplace=True)

# Plot Precision, Recall, and F1-Score for each class with improved clarity
plt.figure(figsize=(15, 8))

for i, metric in enumerate(metrics):
    plt.subplot(1, 3, i + 1)
    sns.barplot(x='Class', y=metric, hue='Model', data=combined_df, palette=['skyblue', 'lightgreen'])
    plt.title(f'{metric.capitalize()} Comparison by Class')
    plt.ylim(0, 1)
    plt.legend(title='Model')
    plt.xlabel('Class')
    plt.ylabel(metric.capitalize())

plt.suptitle('Precision, Recall, and F1-Score Comparison for CNN vs ResNet')
plt.tight_layout()
plt.show()

# Confusion Matrices for both models
cnn_conf_matrix = confusion_matrix(true_classes, cnn_pred_classes)
resnet_conf_matrix = confusion_matrix(true_classes, resnet_pred_classes)

# Plotting confusion matrices with improved visuals
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
sns.heatmap(cnn_conf_matrix, annot=True, fmt="d", cmap="Blues", ax=axes[0],
            xticklabels=class_labels, yticklabels=class_labels, cbar=False)
axes[0].set_title("CNN Confusion Matrix")
axes[0].set_xlabel("Predicted Labels")
axes[0].set_ylabel("True Labels")

sns.heatmap(resnet_conf_matrix, annot=True, fmt="d", cmap="Greens", ax=axes[1],
            xticklabels=class_labels, yticklabels=class_labels, cbar=False)
axes[1].set_title("ResNet Confusion Matrix")
axes[1].set_xlabel("Predicted Labels")
axes[1].set_ylabel("True Labels")

plt.suptitle("Confusion Matrix Comparison")
plt.tight_layout()
plt.show()

# Plotting overall accuracy comparison with adjusted scaling
plt.figure(figsize=(10, 6))
models = ['CNN', 'ResNet']
accuracy_values = [cnn_accuracy, resnet_accuracy]
colors = ['skyblue', 'lightgreen']
plt.bar(models, accuracy_values, color=colors)
plt.title('Overall Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)  # Standard scale to 100%
for i, v in enumerate(accuracy_values):
    plt.text(i, v + 0.02, f"{v * 100:.2f}%", ha='center', fontweight='bold')
plt.show()

# Support comparison for each class (number of instances per class in test set)
plt.figure(figsize=(10, 6))
support_values = [cnn_df[cnn_df['index'] == 'Fake']['support'].values[0],
                 cnn_df[cnn_df['index'] == 'Real']['support'].values[0]]
plt.bar(class_labels, support_values, color=['coral', 'limegreen'])
plt.title("Support (Number of Instances) per Class in Test Set")
plt.xlabel("Class")
plt.ylabel("Support (Instances)")
for i, v in enumerate(support_values):
    plt.text(i, v + 10, str(int(v)), ha='center', fontweight='bold')
plt.show()